{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Examples!  ðŸ’•\n",
    "\n",
    "ðŸš€ Extend your pyspark powers with pyspark+ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup [Optional]\n",
    "<details class=\"tip\">\n",
    "\n",
    "<summary>Create a virtualenv ðŸ”§ </summary>\n",
    "Create a new virtualenv before start this notebook to be able to select it as the kernel, if you want!\n",
    "\n",
    "* Create a new virtualenv.\n",
    "\n",
    "```\n",
    "  pyenv virtualenv 3.9.16 .envPysparkPlus\n",
    "  pyenv activate .envPysparkPlus\n",
    "  pip install --upgrade pip\n",
    "  pip install ipykernel\n",
    "  ```\n",
    "\n",
    "* Delete the virtualenv.\n",
    "\n",
    "  ```\n",
    "  pyenv deactivate .envPysparkPlus\n",
    "  pyenv virtualenv-delete -f .envPysparkPlus\n",
    "  ```\n",
    "\n",
    "* Should return empty\n",
    "\n",
    "  ```\n",
    "  pyenv versions | grep .envPysparkPlus\n",
    "  ``` \n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required! ðŸ’¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pysparkplus in /home/silvio/.pyenv/versions/3.9.16/envs/.envPysparkPlus/lib/python3.9/site-packages (0.0.2)\n",
      "Requirement already satisfied: pyspark<4.0.0,>=3.4.0 in /home/silvio/.pyenv/versions/3.9.16/envs/.envPysparkPlus/lib/python3.9/site-packages (from pysparkplus) (3.4.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/silvio/.pyenv/versions/3.9.16/envs/.envPysparkPlus/lib/python3.9/site-packages (from pyspark<4.0.0,>=3.4.0->pysparkplus) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pysparkplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/04/22 14:05:11 WARN Utils: Your hostname, DESKTOP-O03M3NM resolves to a loopback address: 127.0.1.1; using 172.17.155.166 instead (on interface eth0)\n",
      "23/04/22 14:05:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/22 14:05:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"testPysparkPlus\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple deduplicate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysparkplus.functions import deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Rose|\n",
      "|Rose|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([{\"name\":\"Rose\"}, {\"name\":\"Rose\"}])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Rose|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dedup = deduplicate(df, by_columns=\"name\")\n",
    "df_dedup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 10|Rose|\n",
      "|  5|Rose|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_two_cols = spark.createDataFrame([{\"name\":\"Rose\", \"age\":10}, {\"name\":\"Rose\", \"age\":5}])\n",
    "df_two_cols.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 10|Rose|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_two_dedup = deduplicate(df_two_cols, by_columns=\"name\")\n",
    "df_two_dedup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 10|Rose|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_two_dedup = deduplicate(df_two_cols, by_columns=\"name\", order_by=\"age\")\n",
    "df_two_dedup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1|   a|\n",
      "|   2|   b|\n",
      "|   1|   a|\n",
      "|   3|   c|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"a\"), (2, \"b\"), (1, \"a\"), (3, \"c\")], [\"col1\", \"col2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1|   a|\n",
      "|   2|   b|\n",
      "|   3|   c|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dedup = deduplicate(df, \"col1\")\n",
    "df_dedup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1|   a|\n",
      "|   2|   b|\n",
      "|   3|   c|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dedup = deduplicate(df, [\"col1\", \"col2\"], order_by=\"col1\")\n",
    "df_dedup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run project local ðŸ“€\n",
    "import os \n",
    "import sys \n",
    "sys.path.insert(0, os.path.abspath(\"../..\"))\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysparkplus-exZQCaoz-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
