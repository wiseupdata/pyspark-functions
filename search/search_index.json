{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Extend your pyspark powers with pyspark+","text":"<p>Features \u2728\ufe0f</p> <ul> <li>Wrapper Class!</li> <li>Simple use!</li> <li>Made with A.I. contribution \ud83e\udd16 </li> </ul>"},{"location":"#main-class","title":"Main Class: \ud83d\ude80","text":""},{"location":"#pysparkplus.functions.deduplicate","title":"<code>deduplicate(df, by_columns, order_by=None)</code>","text":"<p>Returns a DataFrame with duplicate rows removed based on the given columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pyspark.sql.DataFrame</code> <p>The input DataFrame.</p> required <code>by_columns</code> <code>Union[str, List[str]]</code> <p>A column or list of columns to group by for deduplication.</p> required <code>order_by</code> <code>Optional[Union[str, List[str]]]</code> <p>A column or list of columns to order by before deduplication. If not specified, the deduplication will be performed based on the <code>by_columns</code> parameter.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame with duplicate rows removed.</p> <p>Deduplicating a DataFrame</p> <p>This example shows how to use <code>deduplicate()</code> to remove duplicate rows from a DataFrame.</p> Original dfExample 1Example 2 <p><pre><code>df = spark.createDataFrame([(1, \"a\"), (2, \"b\"), (1, \"a\"), (3, \"c\")], [\"col1\", \"col2\"])\ndf.show()\n</code></pre> Output: <pre><code>+----+----+\n|col1|col2|\n+----+----+\n|   1|   a|\n|   2|   b|\n|   1|   a|\n|   3|   c|\n+----+----+\n</code></pre></p> <p><pre><code>df_dedup = deduplicate(df, \"col1\")\ndf_dedup.show()\n</code></pre> Output: <pre><code>+----+----+\n|col1|col2|\n+----+----+\n|   1|   a|\n|   2|   b|\n|   3|   c|\n+----+----+\n</code></pre></p> <p><pre><code>df_dedup = deduplicate(df, [\"col1\", \"col2\"], order_by=\"col1\")\ndf_dedup.show()\n</code></pre> Output: <pre><code>+----+----+\n|col1|col2|\n+----+----+\n|   1|   a|\n|   2|   b|\n|   3|   c|\n+----+----+\n</code></pre></p> Important <ul> <li>This function preserves the first occurrence of each unique row and removes subsequent duplicates.</li> <li>If there are no duplicate rows in the DataFrame, this function returns the input DataFrame unchanged.</li> <li>The <code>order_by</code> parameter can be used to specify a custom order for the deduplication. By default, the function   orders by the columns specified in the <code>by_columns</code> parameter.</li> <li>The input DataFrame should not contain null values, as these may cause unexpected behavior in the deduplication.</li> </ul> Source code in <code>pysparkplus/functions.py</code> <pre><code>def deduplicate(df: DataFrame, by_columns, order_by=None) -&gt; DataFrame:\n\"\"\"\n\n    Returns a DataFrame with duplicate rows removed based on the given columns.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input DataFrame.\n        by_columns (Union[str, List[str]]): A column or list of columns to group by for deduplication.\n        order_by (Optional[Union[str, List[str]]]): A column or list of columns to order by before deduplication. If not\n            specified, the deduplication will be performed based on the `by_columns` parameter.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame with duplicate rows removed.\n\n    !!! Example \"Deduplicating a DataFrame\"\n        This example shows how to use `deduplicate()` to remove duplicate rows from a DataFrame.\n\n        === \"Original df\"\n            ```python\n            df = spark.createDataFrame([(1, \"a\"), (2, \"b\"), (1, \"a\"), (3, \"c\")], [\"col1\", \"col2\"])\n            df.show()\n            ```\n            Output:\n            ```\n            +----+----+\n            |col1|col2|\n            +----+----+\n            |   1|   a|\n            |   2|   b|\n            |   1|   a|\n            |   3|   c|\n            +----+----+\n            ```\n\n        === \"Example 1\"\n            ```python\n            df_dedup = deduplicate(df, \"col1\")\n            df_dedup.show()\n            ```\n            Output:\n            ```\n            +----+----+\n            |col1|col2|\n            +----+----+\n            |   1|   a|\n            |   2|   b|\n            |   3|   c|\n            +----+----+\n            ```\n\n        === \"Example 2\"\n            ```python\n            df_dedup = deduplicate(df, [\"col1\", \"col2\"], order_by=\"col1\")\n            df_dedup.show()\n            ```\n            Output:\n            ```\n            +----+----+\n            |col1|col2|\n            +----+----+\n            |   1|   a|\n            |   2|   b|\n            |   3|   c|\n            +----+----+\n            ```\n\n    Info: Important\n        - This function preserves the first occurrence of each unique row and removes subsequent duplicates.\n        - If there are no duplicate rows in the DataFrame, this function returns the input DataFrame unchanged.\n        - The `order_by` parameter can be used to specify a custom order for the deduplication. By default, the function\n          orders by the columns specified in the `by_columns` parameter.\n        - The input DataFrame should not contain null values, as these may cause unexpected behavior in the deduplication.\n    \"\"\"\n    order_by = by_columns if order_by is None else order_by\n\n    order_by = Str(order_by) if isinstance(order_by, str) else order_by\n\n    if len(df.columns) == 1:\n        return df.distinct()\n\n    else:\n        order_by = \",\".join(order_by) if isinstance(order_by, list) else order_by\n\n        # Create a window specification to partition by key columns and order by order columns in descending order\n        window_spec = Window.partitionBy(by_columns).orderBy(desc(order_by))\n\n        # Add a new column called \"row_num\" to the DataFrame based on the window specification\n        df_num = df.withColumn(\"row_num\", row_number().over(window_spec))\n\n        # Filter the DataFrame to keep only rows where the \"row_num\" column equals 1\n        df_dedup = df_num.filter(df_num.row_num == 1)\n\n        return df_dedup.drop(\"row_num\")\n</code></pre>"},{"location":"examples/functions_01/","title":"Functions Examples!  \ud83d\udc95","text":"In\u00a0[2]: Copied! <pre>pip install pysparkplus\n</pre> pip install pysparkplus <pre>Requirement already satisfied: pysparkplus in /home/silvio/.pyenv/versions/3.9.16/envs/.envPysparkPlus/lib/python3.9/site-packages (0.0.2)\nRequirement already satisfied: pyspark&lt;4.0.0,&gt;=3.4.0 in /home/silvio/.pyenv/versions/3.9.16/envs/.envPysparkPlus/lib/python3.9/site-packages (from pysparkplus) (3.4.0)\nRequirement already satisfied: py4j==0.10.9.7 in /home/silvio/.pyenv/versions/3.9.16/envs/.envPysparkPlus/lib/python3.9/site-packages (from pyspark&lt;4.0.0,&gt;=3.4.0-&gt;pysparkplus) (0.10.9.7)\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[3]: Copied! <pre>from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"testPysparkPlus\").getOrCreate()\n</pre> from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"testPysparkPlus\").getOrCreate() <pre>your 131072x1 screen size is bogus. expect trouble\n23/04/22 14:05:11 WARN Utils: Your hostname, DESKTOP-O03M3NM resolves to a loopback address: 127.0.1.1; using 172.17.155.166 instead (on interface eth0)\n23/04/22 14:05:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/04/22 14:05:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n</pre> In\u00a0[4]: Copied! <pre>from pysparkplus.functions import deduplicate\n</pre> from pysparkplus.functions import deduplicate In\u00a0[5]: Copied! <pre>df = spark.createDataFrame([{\"name\":\"Rose\"}, {\"name\":\"Rose\"}])\ndf.show()\n</pre> df = spark.createDataFrame([{\"name\":\"Rose\"}, {\"name\":\"Rose\"}]) df.show() <pre>                                                                                \r</pre> <pre>+----+\n|name|\n+----+\n|Rose|\n|Rose|\n+----+\n\n</pre> In\u00a0[6]: Copied! <pre>df_dedup = deduplicate(df, by_columns=\"name\")\ndf_dedup.show()\n</pre> df_dedup = deduplicate(df, by_columns=\"name\") df_dedup.show() <pre>+----+\n|name|\n+----+\n|Rose|\n+----+\n\n</pre> In\u00a0[7]: Copied! <pre>df_two_cols = spark.createDataFrame([{\"name\":\"Rose\", \"age\":10}, {\"name\":\"Rose\", \"age\":5}])\ndf_two_cols.show()\n</pre> df_two_cols = spark.createDataFrame([{\"name\":\"Rose\", \"age\":10}, {\"name\":\"Rose\", \"age\":5}]) df_two_cols.show() <pre>+---+----+\n|age|name|\n+---+----+\n| 10|Rose|\n|  5|Rose|\n+---+----+\n\n</pre> In\u00a0[8]: Copied! <pre>df_two_dedup = deduplicate(df_two_cols, by_columns=\"name\")\ndf_two_dedup.show()\n</pre> df_two_dedup = deduplicate(df_two_cols, by_columns=\"name\") df_two_dedup.show() <pre>+---+----+\n|age|name|\n+---+----+\n| 10|Rose|\n+---+----+\n\n</pre> In\u00a0[9]: Copied! <pre>df_two_dedup = deduplicate(df_two_cols, by_columns=\"name\", order_by=\"age\")\ndf_two_dedup.show()\n</pre> df_two_dedup = deduplicate(df_two_cols, by_columns=\"name\", order_by=\"age\") df_two_dedup.show() <pre>+---+----+\n|age|name|\n+---+----+\n| 10|Rose|\n+---+----+\n\n</pre> In\u00a0[10]: Copied! <pre>df = spark.createDataFrame([(1, \"a\"), (2, \"b\"), (1, \"a\"), (3, \"c\")], [\"col1\", \"col2\"])\ndf.show()\n</pre> df = spark.createDataFrame([(1, \"a\"), (2, \"b\"), (1, \"a\"), (3, \"c\")], [\"col1\", \"col2\"]) df.show() <pre>+----+----+\n|col1|col2|\n+----+----+\n|   1|   a|\n|   2|   b|\n|   1|   a|\n|   3|   c|\n+----+----+\n\n</pre> In\u00a0[11]: Copied! <pre>df_dedup = deduplicate(df, \"col1\")\ndf_dedup.show()\n</pre> df_dedup = deduplicate(df, \"col1\") df_dedup.show() <pre>+----+----+\n|col1|col2|\n+----+----+\n|   1|   a|\n|   2|   b|\n|   3|   c|\n+----+----+\n\n</pre> In\u00a0[12]: Copied! <pre>df_dedup = deduplicate(df, [\"col1\", \"col2\"], order_by=\"col1\")\ndf_dedup.show()\n</pre> df_dedup = deduplicate(df, [\"col1\", \"col2\"], order_by=\"col1\") df_dedup.show() <pre>+----+----+\n|col1|col2|\n+----+----+\n|   1|   a|\n|   2|   b|\n|   3|   c|\n+----+----+\n\n</pre> In\u00a0[13]: Copied! <pre>### Run project local \ud83d\udcc0\nimport os \nimport sys \nsys.path.insert(0, os.path.abspath(\"../..\"))\nsys.path.insert(0, os.path.abspath(\"..\"))\n</pre> ### Run project local \ud83d\udcc0 import os  import sys  sys.path.insert(0, os.path.abspath(\"../..\")) sys.path.insert(0, os.path.abspath(\"..\"))"},{"location":"examples/functions_01/#functions-examples","title":"Functions Examples!  \ud83d\udc95\u00b6","text":"<p>\ud83d\ude80 Extend your pyspark powers with pyspark+</p>"},{"location":"examples/functions_01/#setup-optional","title":"Setup [Optional]\u00b6","text":"Create a virtualenv \ud83d\udd27  Create a new virtualenv before start this notebook to be able to select it as the kernel, if you want! <ul> <li>Create a new virtualenv.</li> </ul> <pre><code>  pyenv virtualenv 3.9.16 .envPysparkPlus\n  pyenv activate .envPysparkPlus\n  pip install --upgrade pip\n  pip install ipykernel\n</code></pre> <ul> <li><p>Delete the virtualenv.</p> <pre><code>pyenv deactivate .envPysparkPlus\npyenv virtualenv-delete -f .envPysparkPlus\n</code></pre> </li> <li><p>Should return empty</p> <pre><code>pyenv versions | grep .envPysparkPlus\n</code></pre> </li> </ul>"},{"location":"examples/functions_01/#required","title":"Required! \ud83d\udca2\u00b6","text":""},{"location":"examples/functions_01/#simple-deduplicate","title":"Simple deduplicate!\u00b6","text":""},{"location":"pysparkplus/SUMMARY/","title":"package","text":"<ul> <li>functions</li> </ul>"},{"location":"pysparkplus/functions/","title":"Functions","text":""},{"location":"pysparkplus/functions/#pysparkplus.functions.deduplicate","title":"<code>deduplicate(df, by_columns, order_by=None)</code>","text":"<p>Returns a DataFrame with duplicate rows removed based on the given columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pyspark.sql.DataFrame</code> <p>The input DataFrame.</p> required <code>by_columns</code> <code>Union[str, List[str]]</code> <p>A column or list of columns to group by for deduplication.</p> required <code>order_by</code> <code>Optional[Union[str, List[str]]]</code> <p>A column or list of columns to order by before deduplication. If not specified, the deduplication will be performed based on the <code>by_columns</code> parameter.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame with duplicate rows removed.</p> <p>Deduplicating a DataFrame</p> <p>This example shows how to use <code>deduplicate()</code> to remove duplicate rows from a DataFrame.</p> Original dfExample 1Example 2 <p><pre><code>df = spark.createDataFrame([(1, \"a\"), (2, \"b\"), (1, \"a\"), (3, \"c\")], [\"col1\", \"col2\"])\ndf.show()\n</code></pre> Output: <pre><code>+----+----+\n|col1|col2|\n+----+----+\n|   1|   a|\n|   2|   b|\n|   1|   a|\n|   3|   c|\n+----+----+\n</code></pre></p> <p><pre><code>df_dedup = deduplicate(df, \"col1\")\ndf_dedup.show()\n</code></pre> Output: <pre><code>+----+----+\n|col1|col2|\n+----+----+\n|   1|   a|\n|   2|   b|\n|   3|   c|\n+----+----+\n</code></pre></p> <p><pre><code>df_dedup = deduplicate(df, [\"col1\", \"col2\"], order_by=\"col1\")\ndf_dedup.show()\n</code></pre> Output: <pre><code>+----+----+\n|col1|col2|\n+----+----+\n|   1|   a|\n|   2|   b|\n|   3|   c|\n+----+----+\n</code></pre></p> Important <ul> <li>This function preserves the first occurrence of each unique row and removes subsequent duplicates.</li> <li>If there are no duplicate rows in the DataFrame, this function returns the input DataFrame unchanged.</li> <li>The <code>order_by</code> parameter can be used to specify a custom order for the deduplication. By default, the function   orders by the columns specified in the <code>by_columns</code> parameter.</li> <li>The input DataFrame should not contain null values, as these may cause unexpected behavior in the deduplication.</li> </ul> Source code in <code>pysparkplus/functions.py</code> <pre><code>def deduplicate(df: DataFrame, by_columns, order_by=None) -&gt; DataFrame:\n\"\"\"\n\n    Returns a DataFrame with duplicate rows removed based on the given columns.\n\n    Args:\n        df (pyspark.sql.DataFrame): The input DataFrame.\n        by_columns (Union[str, List[str]]): A column or list of columns to group by for deduplication.\n        order_by (Optional[Union[str, List[str]]]): A column or list of columns to order by before deduplication. If not\n            specified, the deduplication will be performed based on the `by_columns` parameter.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame with duplicate rows removed.\n\n    !!! Example \"Deduplicating a DataFrame\"\n        This example shows how to use `deduplicate()` to remove duplicate rows from a DataFrame.\n\n        === \"Original df\"\n            ```python\n            df = spark.createDataFrame([(1, \"a\"), (2, \"b\"), (1, \"a\"), (3, \"c\")], [\"col1\", \"col2\"])\n            df.show()\n            ```\n            Output:\n            ```\n            +----+----+\n            |col1|col2|\n            +----+----+\n            |   1|   a|\n            |   2|   b|\n            |   1|   a|\n            |   3|   c|\n            +----+----+\n            ```\n\n        === \"Example 1\"\n            ```python\n            df_dedup = deduplicate(df, \"col1\")\n            df_dedup.show()\n            ```\n            Output:\n            ```\n            +----+----+\n            |col1|col2|\n            +----+----+\n            |   1|   a|\n            |   2|   b|\n            |   3|   c|\n            +----+----+\n            ```\n\n        === \"Example 2\"\n            ```python\n            df_dedup = deduplicate(df, [\"col1\", \"col2\"], order_by=\"col1\")\n            df_dedup.show()\n            ```\n            Output:\n            ```\n            +----+----+\n            |col1|col2|\n            +----+----+\n            |   1|   a|\n            |   2|   b|\n            |   3|   c|\n            +----+----+\n            ```\n\n    Info: Important\n        - This function preserves the first occurrence of each unique row and removes subsequent duplicates.\n        - If there are no duplicate rows in the DataFrame, this function returns the input DataFrame unchanged.\n        - The `order_by` parameter can be used to specify a custom order for the deduplication. By default, the function\n          orders by the columns specified in the `by_columns` parameter.\n        - The input DataFrame should not contain null values, as these may cause unexpected behavior in the deduplication.\n    \"\"\"\n    order_by = by_columns if order_by is None else order_by\n\n    order_by = Str(order_by) if isinstance(order_by, str) else order_by\n\n    if len(df.columns) == 1:\n        return df.distinct()\n\n    else:\n        order_by = \",\".join(order_by) if isinstance(order_by, list) else order_by\n\n        # Create a window specification to partition by key columns and order by order columns in descending order\n        window_spec = Window.partitionBy(by_columns).orderBy(desc(order_by))\n\n        # Add a new column called \"row_num\" to the DataFrame based on the window specification\n        df_num = df.withColumn(\"row_num\", row_number().over(window_spec))\n\n        # Filter the DataFrame to keep only rows where the \"row_num\" column equals 1\n        df_dedup = df_num.filter(df_num.row_num == 1)\n\n        return df_dedup.drop(\"row_num\")\n</code></pre>"}]}