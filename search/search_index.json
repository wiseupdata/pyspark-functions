{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Extend your pyspark powers with pyspark+","text":"<p>Features \u2728\ufe0f</p> <ul> <li>Wrapper Class</li> <li>Made with A.I. contribution \ud83e\udd16 </li> </ul>"},{"location":"#main-class","title":"Main Class: \ud83d\ude80","text":""},{"location":"examples/functions_01/","title":"Functions Examples!  \ud83d\udc95","text":"In\u00a0[2]: Copied! <pre>pip install pysparkplus\n</pre> pip install pysparkplus <pre>Collecting pysparkplus\n  Downloading pysparkplus-1.0.6-py3-none-any.whl (7.3 kB)\nInstalling collected packages: pysparkplus\nSuccessfully installed pysparkplus-1.0.6\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[1]: Copied! <pre>from pysparkplus import Str\n</pre> from pysparkplus import Str In\u00a0[2]: Copied! <pre>my_new_string = Str(\"Print me please, simple and easy!\")\n\nmy_new_string.print\n</pre> my_new_string = Str(\"Print me please, simple and easy!\")  my_new_string.print <pre>Print me please, simple and easy!\n</pre> In\u00a0[3]: Copied! <pre>for word in my_new_string.list:\n    word.print\n</pre> for word in my_new_string.list:     word.print <pre>Print\nme\nplease\nsimple\nand\neasy\n</pre> In\u00a0[4]: Copied! <pre>### Run project local \ud83d\udcc0\n# import os \n# import sys \n# sys.path.insert(0, os.path.abspath(\"../..\"))\n# sys.path.insert(0, os.path.abspath(\"..\"))\n</pre> ### Run project local \ud83d\udcc0 # import os  # import sys  # sys.path.insert(0, os.path.abspath(\"../..\")) # sys.path.insert(0, os.path.abspath(\"..\"))"},{"location":"examples/functions_01/#functions-examples","title":"Functions Examples!  \ud83d\udc95\u00b6","text":"<p>\ud83d\ude80 Extend your pyspark powers with pyspark+</p>"},{"location":"examples/functions_01/#setup-optional","title":"Setup [Optional]\u00b6","text":"Create a virtualenv \ud83d\udd27  Create a new virtualenv before start this notebook to be able to select it as the kernel, if you want! <ul> <li>Create a new virtualenv.</li> </ul> <pre><code>  pyenv virtualenv 3.9.16 .envpysparkplus\n  pyenv activate .envPysparkPlus\n  pip install --upgrade pip\n  pip install ipykernel\n</code></pre> <ul> <li><p>Delete the virtualenv.</p> <pre><code>pyenv deactivate .envPysparkPlus\npyenv virtualenv-delete -f .envPysparkPlus\n</code></pre> </li> <li><p>Should return empty</p> <pre><code>pyenv versions | grep .envPysparkPlus\n</code></pre> </li> </ul>"},{"location":"examples/functions_01/#required","title":"Required! \ud83d\udca2\u00b6","text":""},{"location":"examples/functions_01/#print-inside","title":"Print inside!\u00b6","text":""},{"location":"pysparkplus/SUMMARY/","title":"package","text":"<ul> <li>dataframe</li> <li>main</li> </ul>"},{"location":"pysparkplus/dataframe/","title":"Dataframe","text":""},{"location":"pysparkplus/dataframe/#pysparkplus.dataframe.deduplicate","title":"<code>deduplicate(dataframe, key_columns, order_columns)</code>","text":"<p>This function removes duplicates from a Spark DataFrame based on a combination of columns, while retaining only the rows with the highest values in a specified column.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The input Spark DataFrame to be deduplicated.</p> required <code>key_columns</code> <code>List[str]</code> <p>The list of column names to be used as keys for deduplication.</p> required <code>order_columns</code> <code>List[str]</code> <p>The list of column names to be used for sorting rows in descending order.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new Spark DataFrame that removes duplicates based on the values in the key_columns</p> <code>DataFrame</code> <p>and keeps only the row with the highest value in the order_columns.</p> Source code in <code>pysparkplus/dataframe.py</code> <pre><code>def deduplicate(dataframe: DataFrame, key_columns: List[str], order_columns: List[str]) -&gt; DataFrame:\n\"\"\"\n    This function removes duplicates from a Spark DataFrame based on a combination of columns,\n    while retaining only the rows with the highest values in a specified column.\n\n    Args:\n        dataframe (DataFrame): The input Spark DataFrame to be deduplicated.\n        key_columns (List[str]): The list of column names to be used as keys for deduplication.\n        order_columns (List[str]): The list of column names to be used for sorting rows in descending order.\n\n    Returns:\n        DataFrame: A new Spark DataFrame that removes duplicates based on the values in the key_columns\n        and keeps only the row with the highest value in the order_columns.\n    \"\"\"\n    # Check if input parameters are valid\n    if not isinstance(dataframe, DataFrame):\n        raise ValueError(\"Input parameter 'dataframe' should be of type DataFrame.\")\n    if not isinstance(key_columns, list):\n        raise ValueError(\"Input parameter 'key_columns' should be of type List[str].\")\n    if not isinstance(order_columns, list):\n        raise ValueError(\"Input parameter 'order_columns' should be of type List[str].\")\n    if len(key_columns) == 0:\n        raise ValueError(\"Input parameter 'key_columns' should not be an empty list.\")\n    if len(order_columns) == 0:\n        raise ValueError(\"Input parameter 'order_columns' should not be an empty list.\")\n\n    # Create a window specification to partition by key columns and order by order columns in descending order\n    window_spec = Window.partitionBy(*key_columns).orderBy(desc(*order_columns))\n\n    # Add a new column called \"row_num\" to the DataFrame based on the window specification\n    deduplicated_dataframe = dataframe.withColumn(\"row_num\", row_number().over(window_spec))\n\n    # Filter the DataFrame to keep only rows where the \"row_num\" column equals 1\n    deduplicated_dataframe = deduplicated_dataframe.filter(deduplicated_dataframe.row_num == 1)\n\n    # Drop the \"row_num\" column from the DataFrame\n    deduplicated_dataframe = deduplicated_dataframe.drop(\"row_num\")\n\n    return deduplicated_dataframe\n</code></pre>"},{"location":"pysparkplus/main/","title":"Main","text":""}]}